{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "r1TWxwhTJJ9N",
        "InZdBFpqJR8t",
        "bPaIc2slJhdz",
        "Skyqgin3J_Ep",
        "QJdQZjq0LVBl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZQb2mU08OG5",
        "outputId": "f917fa27-2895-4013-e489-b45074e41acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "r1TWxwhTJJ9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJmGLFuZIwWp"
      },
      "outputs": [],
      "source": [
        "!pip -qq install langchain langchain_community langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install langchain_openai"
      ],
      "metadata": {
        "id": "c1Osp2c_JkqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install python-docx pypdf PyPDF2"
      ],
      "metadata": {
        "id": "J8_DFxumJozH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install sentence_transformers"
      ],
      "metadata": {
        "id": "ymj0YlExKr3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##import packages and pincone api key setting\n"
      ],
      "metadata": {
        "id": "InZdBFpqJR8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "import os\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"Enter Your API Key\""
      ],
      "metadata": {
        "id": "F4Ck1blSJa7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sentence transformers to chunk the data by using model from huggingface"
      ],
      "metadata": {
        "id": "bPaIc2slJhdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_book(book_path):\n",
        "    loader = PyPDFLoader(book_path)\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "#create chunks of text\n",
        "def split_text(extracted_data):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "    text_chunks = text_splitter.split_documents(extracted_data)\n",
        "    return text_chunks\n",
        "\n",
        "#download embedding model\n",
        "def download_embedding_model():\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return embedding_model"
      ],
      "metadata": {
        "id": "NhlHfMFYJ9az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chating with LLM from pincone knowledge"
      ],
      "metadata": {
        "id": "Skyqgin3J_Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data = load_data_from_book(\"/content/drive/MyDrive/FinalYearProject/Medical_book.pdf\")\n",
        "text_chunks = split_text(extracted_data)\n",
        "print(len(text_chunks))\n",
        "embedding = download_embedding_model()\n",
        "index_name = \"medical-chatbot\"\n",
        "\n",
        "# Initialize a LangChain embedding object.\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "embeddings = PineconeEmbeddings(\n",
        "    model=model_name,\n",
        "    pinecone_api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
        ")\n",
        "\n",
        "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
        "docsearch = PineconeVectorStore.from_documents(\n",
        "    documents=text_chunks,\n",
        "    index_name=index_name,\n",
        "    embedding=embedding,\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",\n",
        "openai_api_key=\"Enter Your API Key\")\n",
        "\n",
        "# Initialize a LangChain object for chatting with the LLM\n",
        "# with knowledge from Pinecone.\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxuMG-ZpKZX0",
        "outputId": "f4432d21-b608-4ed0-c824-2af49982cd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7b25c37e8c40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Interface"
      ],
      "metadata": {
        "id": "QJdQZjq0LVBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install pyngrok"
      ],
      "metadata": {
        "id": "2u6HvI50K_Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVUFx2GWtPkb",
        "outputId": "48fd5eda-c275-4b41-f22e-193bec987ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-06 03:27:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 35.71.179.82, 13.248.244.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  7.76MB/s    in 1.7s    \n",
            "\n",
            "2024-11-06 03:27:04 (7.76 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken \"Enter Your ngork authtoken\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogcyowUBtlaR",
        "outputId": "7b794083-eae5-4738-eef1-a12a882fac54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip the user interface files\n",
        "!unzip -qq /content/drive/MyDrive/FinalYearProject/templates.zip\n"
      ],
      "metadata": {
        "id": "P8mnikr0SHt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18900a8-d38a-4caa-9267-016d905bf61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace templates/chat_bot.html? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the upload folder and allowed file extensions\n",
        "UPLOAD_FOLDER = 'uploads/'\n",
        "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'docx'}\n",
        "\n",
        "# Check if the uploaded file has an allowed extension\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "# Function to read a text file\n",
        "def read_txt(file):\n",
        "    return file.read().decode('utf-8')\n",
        "\n",
        "# Function to read a PDF file\n",
        "def read_pdf(file):\n",
        "    reader = PdfReader(file)\n",
        "    text = ''\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to read a docx file\n",
        "def read_docx(file):\n",
        "    doc = Document(file)\n",
        "    text = ''\n",
        "    for para in doc.paragraphs:\n",
        "        text += para.text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Replace with your Google Places API key\n",
        "PLACES_API_KEY = \"Enter Your API Key\"\n",
        "\n",
        "# Create the upload folder if it doesn't exist\n",
        "if not os.path.exists(UPLOAD_FOLDER):\n",
        "    os.makedirs(UPLOAD_FOLDER)"
      ],
      "metadata": {
        "id": "bcFy1c75QhaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request, flash, jsonify\n",
        "import requests\n",
        "from werkzeug.utils import secure_filename\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "\n",
        "#home page\n",
        "@app.route('/')\n",
        "@app.route('/home')\n",
        "def home():\n",
        "    global health_tips\n",
        "    # Static health tips data\n",
        "    health_tips = [\n",
        "        {\"title\": \"Stay Hydrated\", \"text\": \"Drinking enough water is crucial for maintaining overall health. Aim for at least 8 glasses of water a day.\"},\n",
        "        {\"title\": \"Regular Exercise\", \"text\": \"Engage in at least 30 minutes of moderate exercise most days of the week to keep your body fit and healthy.\"},\n",
        "        {\"title\": \"Balanced Diet\", \"text\": \"Include a variety of fruits, vegetables, whole grains, and lean proteins in your diet to ensure you get all essential nutrients.\"}\n",
        "    ]\n",
        "\n",
        "    return render_template('home.html', health_tips=health_tips)\n",
        "\n",
        "@app.route('/get_hospitals', methods=['POST'])\n",
        "def get_hospitals():\n",
        "    data = request.get_json()\n",
        "    latitude = data.get('latitude')\n",
        "    longitude = data.get('longitude')\n",
        "    radius = 5000  # Search radius in meters\n",
        "\n",
        "    # geoapify API request\n",
        "    places_url = f\"https://api.geoapify.com/v2/places?categories=healthcare&filter=circle:{longitude},{latitude},{radius}&limit=10&apiKey={PLACES_API_KEY}\"\n",
        "    response = requests.get(places_url)\n",
        "    if response.status_code != 200:\n",
        "        flash(f\"Not able to find nearby hospitals due to {response.json()['message']}\", 'warning')\n",
        "\n",
        "    places_data = response.json()\n",
        "\n",
        "    hospitals = []\n",
        "    for place in places_data.get('features', []):\n",
        "        details=place['properties']\n",
        "        c_no='N/A'\n",
        "        if \"contact\" in list(details.keys()):\n",
        "            c_no=details[\"contact\"][\"phone\"]\n",
        "        hospitals.append({\n",
        "            'Name': details.get('name'),\n",
        "            'Address': details.get('formatted'),\n",
        "            'Phone Number': c_no,\n",
        "            'Website':details.get('website','N/A'),\n",
        "            'Opening hours':details.get('opening_hours','N/A')\n",
        "        })\n",
        "\n",
        "    return jsonify({'hospitals': hospitals})\n",
        "\n",
        "\n",
        "@app.route('/chatbot')\n",
        "def chatbot():\n",
        "    return render_template('chat_bot.html')\n",
        "\n",
        "# Route to handle message input and optional file upload\n",
        "@app.route('/send_message', methods=['POST'])\n",
        "def send_message():\n",
        "    message = request.form.get('message')  # Get the text input from the user\n",
        "    file = request.files.get('file')  # Get the file if any\n",
        "\n",
        "    file_content,combined_message = \"\",\"\"\n",
        "\n",
        "    # If a file is uploaded, process it based on its type\n",
        "    if file:\n",
        "        filename = file.filename\n",
        "        if filename.endswith('.txt'):\n",
        "            file_content = read_txt(file)\n",
        "        elif filename.endswith('.pdf'):\n",
        "            file_content = read_pdf(file)\n",
        "        elif filename.endswith('.docx'):\n",
        "            file_content = read_docx(file)\n",
        "        else:\n",
        "            return jsonify({'reply': 'Unsupported file type'})\n",
        "\n",
        "    # Combine user message and file content (if any) before passing to LLM\n",
        "    if len(file_content)!=0:\n",
        "        combined_message = f\"\\n\\nFile Content:\\n'{file_content}' from this content answer this question '{message}'\"\n",
        "\n",
        "    response = {}  # Initialize a response dictionary\n",
        "\n",
        "    # Generate a response for the message\n",
        "    if len(combined_message)!=0 and qa:\n",
        "      result = qa.invoke({\"query\":combined_message})\n",
        "      response['reply'] = result[\"result\"]\n",
        "    elif qa:\n",
        "      result=qa.invoke({\"query\":message})\n",
        "      response['reply'] = result[\"result\"]\n",
        "    else:\n",
        "      response['reply'] = \"No response available\"\n",
        "\n",
        "    # Handle file upload if a file was uploaded\n",
        "    if file and allowed_file(file.filename):\n",
        "        filename = secure_filename(file.filename)\n",
        "        file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "        response['file_info'] = f\"File {filename} uploaded successfully.\"\n",
        "    return jsonify(response)\n",
        "\n",
        "# Function to run Flask app\n",
        "def run_flask():\n",
        "    app.run(port=5000)\n",
        "\n",
        "\n",
        "\n",
        "# Expose the Colab environment via ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Your Flask app is available at {public_url}\")\n",
        "\n",
        "# Start Flask app in the background\n",
        "thread = threading.Thread(target=run_flask)\n",
        "thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyarXDK7OZ0Z",
        "outputId": "1863d8fe-870d-4237-9353-aadfd67cd1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Flask app is available at NgrokTunnel: \"https://2344-34-124-131-151.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n"
          ]
        }
      ]
    }
  ]
}